{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Comprehend Topic Modeling \n",
    "\n",
    "You can use Amazon Comprehend to examine the content of a collection of documents to determine common themes. For example, you can give Amazon Comprehend a collection of news articles, and it will determine the subjects, such as sports, politics, or entertainment. The text in the documents doesn't need to be annotated.\n",
    "\n",
    "Amazon Comprehend uses a Latent Dirichlet Allocation-based learning model to determine the topics in a set of documents. It examines each document to determine the context and meaning of a word. The set of words that frequently belong to the same context across the entire document set make up a topic.\n",
    "\n",
    "A word is associated to a topic in a document based on how prevalent that topic is in a document and how much affinity the topic has to the word. The same word can be associated with different topics in different documents based on the topic distribution in a particular document.\n",
    "\n",
    "For example, the word \"glucose\" in an article that talks predominantly about sports can be assigned to the topic \"sports,\" while the same word in an article about \"medicine\" will be assigned to the topic \"medicine.\"\n",
    "\n",
    "Each word associated with a topic is given a weight that indicates how much the word helps define the topic. The weight is an indication of how many times the word occurs in the topic compared to other words in the topic, across the entire document set.\n",
    "\n",
    "For the most accurate results you should provide Amazon Comprehend with the largest possible corpus to work with. For best results:\n",
    "\n",
    "You should use at least 1,000 documents in each topic modeling job.\n",
    "\n",
    "Each document should be at least 3 sentences long.\n",
    "\n",
    "If a document consists of mostly numeric data, you should remove it from the corpus.\n",
    "\n",
    "Topic modeling is an asynchronous process. You submit your list of documents to Amazon Comprehend from an Amazon S3 bucket using the StartTopicsDetectionJob operation. The response is sent to an Amazon S3 bucket. You can configure both the input and output buckets. Get a list of the topic modeling jobs that you have submitted using the ListTopicsDetectionJobs operation and view information about a job using the DescribeTopicsDetectionJob operation. Content delivered to Amazon S3 buckets might contain customer content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab includes step-by-step instructions for topic modeling using Amazon Comprehend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "* AWS region.\n",
    "* The IAM role arn used to give access to Comprehend API and S3 bucket.\n",
    "* The S3 bucket that you want to use for training and model data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import json\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-340280328827/sagemaker/topic-modeling\n"
     ]
    }
   ],
   "source": [
    "prefix = \"sagemaker/topic-modeling\"\n",
    "bucketuri=\"s3://\"+bucket+\"/\"+prefix\n",
    "print(bucketuri)\n",
    "# customize to your bucket where you have stored the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Let's start by uploading the dataset the sample data s3 bucket.The  sample dataset contains Human Activity Recognition Using Smartphones Data Set.\n",
    "The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data. \n",
    "\n",
    "We would be performing topic modeling for these 6 activities \n",
    "\n",
    "More details about dataset: https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones\n",
    "Now lets read this into a Pandas data frame and take a look.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-08-04 21:40:41--  https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 60999314 (58M) [application/x-httpd-php]\n",
      "Saving to: ‘UCI HAR Dataset.zip’\n",
      "\n",
      "UCI HAR Dataset.zip 100%[===================>]  58.17M  18.7MB/s    in 3.1s    \n",
      "\n",
      "2021-08-04 21:40:45 (18.7 MB/s) - ‘UCI HAR Dataset.zip’ saved [60999314/60999314]\n",
      "\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "unzip is already the newest version (6.0-23+deb10u1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 19 not upgraded.\n",
      "Archive:  UCI HAR Dataset.zip\n",
      "   creating: UCI HAR Dataset/\n",
      "  inflating: UCI HAR Dataset/.DS_Store  \n",
      "   creating: __MACOSX/\n",
      "   creating: __MACOSX/UCI HAR Dataset/\n",
      "  inflating: __MACOSX/UCI HAR Dataset/._.DS_Store  \n",
      "  inflating: UCI HAR Dataset/activity_labels.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/._activity_labels.txt  \n",
      "  inflating: UCI HAR Dataset/features.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/._features.txt  \n",
      "  inflating: UCI HAR Dataset/features_info.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/._features_info.txt  \n",
      "  inflating: UCI HAR Dataset/README.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/._README.txt  \n",
      "   creating: UCI HAR Dataset/test/\n",
      "   creating: UCI HAR Dataset/test/Inertial Signals/\n",
      "  inflating: UCI HAR Dataset/test/Inertial Signals/body_acc_x_test.txt  \n",
      "   creating: __MACOSX/UCI HAR Dataset/test/\n",
      "   creating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/\n",
      "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._body_acc_x_test.txt  \n",
      "  inflating: UCI HAR Dataset/test/Inertial Signals/body_acc_y_test.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._body_acc_y_test.txt  \n",
      "  inflating: UCI HAR Dataset/test/Inertial Signals/body_acc_z_test.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._body_acc_z_test.txt  \n",
      "  inflating: UCI HAR Dataset/test/Inertial Signals/body_gyro_x_test.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._body_gyro_x_test.txt  \n",
      "  inflating: UCI HAR Dataset/test/Inertial Signals/body_gyro_y_test.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._body_gyro_y_test.txt  \n",
      "  inflating: UCI HAR Dataset/test/Inertial Signals/body_gyro_z_test.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._body_gyro_z_test.txt  \n",
      "  inflating: UCI HAR Dataset/test/Inertial Signals/total_acc_x_test.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._total_acc_x_test.txt  \n",
      "  inflating: UCI HAR Dataset/test/Inertial Signals/total_acc_y_test.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._total_acc_y_test.txt  \n",
      "  inflating: UCI HAR Dataset/test/Inertial Signals/total_acc_z_test.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._total_acc_z_test.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/test/._Inertial Signals  \n",
      "  inflating: UCI HAR Dataset/test/subject_test.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/test/._subject_test.txt  \n",
      "  inflating: UCI HAR Dataset/test/X_test.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/test/._X_test.txt  \n",
      "  inflating: UCI HAR Dataset/test/y_test.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/test/._y_test.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/._test  \n",
      "   creating: UCI HAR Dataset/train/\n",
      "   creating: UCI HAR Dataset/train/Inertial Signals/\n",
      "  inflating: UCI HAR Dataset/train/Inertial Signals/body_acc_x_train.txt  \n",
      "   creating: __MACOSX/UCI HAR Dataset/train/\n",
      "   creating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/\n",
      "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._body_acc_x_train.txt  \n",
      "  inflating: UCI HAR Dataset/train/Inertial Signals/body_acc_y_train.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._body_acc_y_train.txt  \n",
      "  inflating: UCI HAR Dataset/train/Inertial Signals/body_acc_z_train.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._body_acc_z_train.txt  \n",
      "  inflating: UCI HAR Dataset/train/Inertial Signals/body_gyro_x_train.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._body_gyro_x_train.txt  \n",
      "  inflating: UCI HAR Dataset/train/Inertial Signals/body_gyro_y_train.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._body_gyro_y_train.txt  \n",
      "  inflating: UCI HAR Dataset/train/Inertial Signals/body_gyro_z_train.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._body_gyro_z_train.txt  \n",
      "  inflating: UCI HAR Dataset/train/Inertial Signals/total_acc_x_train.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._total_acc_x_train.txt  \n",
      "  inflating: UCI HAR Dataset/train/Inertial Signals/total_acc_y_train.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._total_acc_y_train.txt  \n",
      "  inflating: UCI HAR Dataset/train/Inertial Signals/total_acc_z_train.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._total_acc_z_train.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/train/._Inertial Signals  \n",
      "  inflating: UCI HAR Dataset/train/subject_train.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/train/._subject_train.txt  \n",
      "  inflating: UCI HAR Dataset/train/X_train.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/train/._X_train.txt  \n",
      "  inflating: UCI HAR Dataset/train/y_train.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/train/._y_train.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/._train  \n",
      "  inflating: __MACOSX/._UCI HAR Dataset  \n"
     ]
    }
   ],
   "source": [
    "# Download the data set\n",
    "\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip\n",
    "!apt-get install unzip -y\n",
    "!unzip -o \"UCI HAR Dataset.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Download the data set\n",
    "\n",
    "!wget https://docs.aws.amazon.com/comprehend/latest/dg/samples/tutorial-reviews-data.zip\n",
    "!apt-get install unzip -y\n",
    "!unzip -o tutorial-reviews-data.zip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                # For matrix operations and numerical processing\n",
    "import pandas as pd \n",
    "\n",
    "# data = pd.read_csv('./amazon-reviews.csv')   \n",
    "read_file = pd.read_csv('./UCI HAR Dataset/test/X_test.txt')\n",
    "data=read_file.to_csv('./Test/human_activity_data.csv', index=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous Batch Processing using StartSentimentDetectionJob\n",
    "\n",
    "To analyze large documents and large collections of documents, use one of the Amazon Comprehend asynchronous operations. There is an asynchronous version of each of the Amazon Comprehend operations and an additional set of operations for topic modeling.\n",
    "\n",
    "To analyze a collection of documents, you typically perform the following steps:\n",
    "\n",
    "   * Store the documents in an Amazon S3 bucket.\n",
    "\n",
    "   * Start one or more jobs to analyze the documents.\n",
    "\n",
    "   * Monitor the progress of an analysis job.\n",
    "\n",
    "   * Retrieve the results of the analysis from an S3 bucket when the job is complete.\n",
    "\n",
    "The following sections describe using the Amazon Comprehend API to run asynchronous operations. \n",
    "\n",
    "We would be using the following API:\n",
    "\n",
    "StartSentimentDetectionJob — Start a job to detect the emotional sentiment in each document in the collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "\n",
    "s3.Bucket(bucket).upload_file(\"./Test/human_activity_data.csv\", \"sagemaker/topic-modeling/human_activity_data.csv\")\n",
    "comprehend = boto3.client('comprehend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "job_uuid = uuid.uuid1()\n",
    "job_name = f\"topicmodeling-job-{job_uuid}\"\n",
    "inputs3uri= bucketuri+\"/HumanActivityRecognition-Test.csv\"\n",
    "asyncresponse = comprehend.start_topics_detection_job(\n",
    "    InputDataConfig={\n",
    "        'S3Uri': inputs3uri,\n",
    "        'InputFormat': 'ONE_DOC_PER_LINE'\n",
    "    },\n",
    "    OutputDataConfig={\n",
    "        'S3Uri': bucketuri,\n",
    "       \n",
    "    },\n",
    "    DataAccessRoleArn=role,\n",
    "    JobName=job_name,\n",
    "    NumberOfTopics=6\n",
    "   \n",
    " \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TopicsDetectionJobProperties': {'JobId': 'c17013807e1b5807b0c46a5335f2dc51', 'JobName': 'topicmodeling-job-6eae0d9c-f56f-11eb-ac13-d558d32189d7', 'JobStatus': 'IN_PROGRESS', 'SubmitTime': datetime.datetime(2021, 8, 4, 22, 0, 51, 260000, tzinfo=tzlocal()), 'InputDataConfig': {'S3Uri': 's3://sagemaker-us-east-1-340280328827/sagemaker/topic-modeling/HumanActivityRecognition-Test.csv', 'InputFormat': 'ONE_DOC_PER_LINE'}, 'OutputDataConfig': {'S3Uri': 's3://sagemaker-us-east-1-340280328827/sagemaker/topic-modeling/340280328827-TOPICS-c17013807e1b5807b0c46a5335f2dc51/output/output.tar.gz'}, 'NumberOfTopics': 6, 'DataAccessRoleArn': 'arn:aws:iam::340280328827:role/SagemakerFullAccessPolicy'}, 'ResponseMetadata': {'RequestId': '178eae27-9e72-45fb-a592-6eeeac5236ba', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '178eae27-9e72-45fb-a592-6eeeac5236ba', 'content-type': 'application/x-amz-json-1.1', 'content-length': '625', 'date': 'Wed, 04 Aug 2021 22:00:52 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "events_job_id = asyncresponse['JobId']\n",
    "job = comprehend.describe_topics_detection_job(JobId=events_job_id)\n",
    "print(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TopicsDetectionJobProperties': {'JobId': 'c17013807e1b5807b0c46a5335f2dc51', 'JobName': 'topicmodeling-job-6eae0d9c-f56f-11eb-ac13-d558d32189d7', 'JobStatus': 'IN_PROGRESS', 'SubmitTime': datetime.datetime(2021, 8, 4, 22, 0, 51, 260000, tzinfo=tzlocal()), 'InputDataConfig': {'S3Uri': 's3://sagemaker-us-east-1-340280328827/sagemaker/topic-modeling/HumanActivityRecognition-Test.csv', 'InputFormat': 'ONE_DOC_PER_LINE'}, 'OutputDataConfig': {'S3Uri': 's3://sagemaker-us-east-1-340280328827/sagemaker/topic-modeling/340280328827-TOPICS-c17013807e1b5807b0c46a5335f2dc51/output/output.tar.gz'}, 'NumberOfTopics': 6, 'DataAccessRoleArn': 'arn:aws:iam::340280328827:role/SagemakerFullAccessPolicy'}, 'ResponseMetadata': {'RequestId': 'c11fde40-9d4f-4536-8892-8254057d0c8f', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'c11fde40-9d4f-4536-8892-8254057d0c8f', 'content-type': 'application/x-amz-json-1.1', 'content-length': '625', 'date': 'Wed, 04 Aug 2021 22:00:56 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "# Get current job status\n",
    "job = comprehend.describe_topics_detection_job(JobId=events_job_id)\n",
    "print(job)\n",
    "# Loop until job is completed\n",
    "waited = 0\n",
    "timeout_minutes = 40\n",
    "while job['TopicsDetectionJobProperties']['JobStatus'] != 'COMPLETED':\n",
    "    sleep(60)\n",
    "    waited += 60\n",
    "    assert waited//60 < timeout_minutes, \"Job timed out after %d seconds.\" % waited\n",
    "    job = comprehend.describe_topics_detection_job(JobId=events_job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job would take roughly 6-8 minutes to complete and you can download the output from the output location you specified in the job paramters. You can open Comprehend in your console and check the job details there as well. Asynchronous method would be very useful when you have multiple documents and you want to run asynchronous batch.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
